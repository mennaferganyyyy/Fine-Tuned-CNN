# -*- coding: utf-8 -*-
"""Untitled72.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yvukT7kgrw-UpGk9I9IIfrJAQJA5frgr
"""

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.applications import VGG16
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint

from google.colab import drive
drive.mount('/content/drive')

import os

path = "/content/drive/MyDrive/data/train"
print("Folders inside train directory:", os.listdir(path))

import os
print("Categories:", os.listdir(data_dir))

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.applications import VGG16
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
import os
from google.colab import drive

# Mount Google Drive
drive.mount('/content/drive')

data_dir = '/content/drive/MyDrive/data/train'  # Change this to your dataset path

import os
print(os.listdir('/content/drive/MyDrive/data'))

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.applications import VGG16
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
import os
from google.colab import drive

# Mount Google Drive
drive.mount('/content/drive')

# Define dataset path
data_dir = '/content/drive/MyDrive/data/train'

# Load pre-trained VGG16 model without the top classification layer
base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))
base_model.trainable = False  # Freeze base model layers initially

# Create a sequential model with fine-tuned layers
model = keras.Sequential([
    base_model,
    layers.GlobalAveragePooling2D(),
    layers.Dense(256, activation='relu', kernel_regularizer=keras.regularizers.l2(0.01)),
    layers.Dropout(0.5),
    layers.Dense(1, activation='sigmoid')  # سيتم تغييره ديناميكيًا بناءً على عدد الفئات
])

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Data augmentation for training
train_datagen = keras.preprocessing.image.ImageDataGenerator(
    rescale=1./255,
    rotation_range=30,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    validation_split=0.2
)

# Load training and validation data
train_generator = train_datagen.flow_from_directory(
    data_dir, target_size=(224, 224), batch_size=32, class_mode='binary', subset='training')

val_generator = train_datagen.flow_from_directory(
    data_dir, target_size=(224, 224), batch_size=32, class_mode='binary', subset='validation')

# Adjust final dense layer for multi-class classification dynamically
num_classes = len(train_generator.class_indices)
if num_classes > 2:
    model.pop()
    model.add(layers.Dense(num_classes, activation='softmax'))
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Callbacks for better training
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
checkpoint = ModelCheckpoint('best_model.h5', save_best_only=True, monitor='val_accuracy')

#  Step 1: Initial training with frozen layers
model.fit(train_generator, validation_data=val_generator, epochs=5, callbacks=[early_stopping, checkpoint])

#  Step 2: Fine-tuning the last 10 layers
base_model.trainable = True
for layer in base_model.layers[:-10]:
    layer.trainable = False

# Recompile model for fine-tuning
model.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-5), loss='categorical_crossentropy' if num_classes > 2 else 'binary_crossentropy', metrics=['accuracy'])

#  Step 3: Train again with fine-tuned layers
model.fit(train_generator, validation_data=val_generator, epochs=5, callbacks=[early_stopping, checkpoint])

# Save final model
model.save('/content/drive/MyDrive/final_cnn_model.keras')













